---
layout: post
title:  "Getting your finances right with PaymentClass"
date:   2016-06-16 20:31:30 -0400
#categories: 
---

<h2>Problem Statement</h2>

I had a great experience working on a data science solution for Cinch Financial. Cinch helps their subscribers to choose the best personal financial products for their needs through an app which combines user financial background, spending history, finance expertise, and unbiased recommendations (Cinch is not paid by any product provider).
Cinch obtains users' spending history from debit/credit card transaction data provided by a third-party vendor. Each transaction has a date and time, a dollar amount, a brief description, and a category assigned by the vendor. For example, if you made a loan payment today, it might look something like "WELLS FARGO HOME BILL PAYM ACH Withdrawal" and be classified as "Mortgage & Rent". The app depends on classifications like this to make sensible recommendations to its users. So what happens if the payment got classified as "Home Services" by mistake? Then in the eyes of the app, you would still carry that debt and have incurred an expenditure. 

Mis-classifications do occur occasionally in the data. Although rare, they can have a significant impact on the reliability of the app. To detect such mis-classifications and correct them, I developed a solution dubbed PaymentClass (it's not the most imaginative title, but at least it has not lost any of its vowels). I was provided with the details of more than 28 million transactions, classified into 120 categories. My approach was to use natural language processing with a number of machine learning models to identify patterns in the descriptions that may accurately predict the right categories.

The data from Cinch presented significant challenges. First of all, it is highly imbalanced across a large number of categories. The charts below show the data point frequencies (blue bars) in the top and bottom 15 categories, which can range from over 4 million to less than 1000. A multi-class approach with bootstrap re-sampling may work in such a scenario, but for another problem. The red bars in the charts show the number of features in each category, obtained by vectorizing unigram tokens generated from the descriptions after pre-processing. While the top 15 categories have nice small features-to-data points ratio, the bottom 15 categories have feature counts that are large compared to the frequencies. That means that the multi-class + bootstrap approach is likely to over-fit some of these categories. Furthermore, the proportion of mis-classified transactions in each category was unknown, although a brief inspection of randomly selected transactions suggest that the majority was correct. Nonetheless, systematic mis-classifications in certain categories may contribute a significant amount of error.

<img src="../images/top15_cat_features.png"><img src="../images/bottom15_cat_features.png">

PaymentClass is divided into two major steps. In the first step, outliers (transactions whose descriptions are least like others in their category) are picked out by a one-class SVM model, which had been trained on the dataset. Outliers are then fed into a second model, which was pre-trained on an external corpus, and a new category is determined. Before each step, a series of pre-processing treatments were applied to the transaction data. Read on for more details on each of these components. Or you could skip to the next section to learn how well this approach worked.

<h2>Solution</h2>

The first step, PaymentClass.Layer1, performs anomaly detection using One-Class SVM using the NLTK and scikit-learn toolkits for pre-processing and model building respectively. This model assumes that the proportion of incorrect classifications is very small (pedantic point - the formulation actually assumes that all training points are correct classifications). Like a regular SVM, the one-class SVM uses a hypersurface in feature space to define the boundary between two distinct categories. The difference is that in one-class SVM, all the training data belong in one category, and the dividing surface is constructed in such a way as to exclude a user-specified proportion of data points, which are then considered as outliers. As you might already have guessed, I had to train a model for each of the 300 categories. Furthermore, each model would have to be separately tuned for accuracy.

In the next step, PaymentClass.Layer2, a new category is assigned to each outlier. My first attempt was to use topic modeling via Latent Dirichlet Allocation with tf-idf weighting. However, it turns out that the topics did not correlate with the categories most of the time. Upon an inspection of a subset of the data, it seemed to me that very few categories actually had transactions with the same words recurring in their descriptions. These tended to be the large categories, such as Transfers and Fast Food, where highly-recurring names of well-known companies dominate in the descriptions. For most categories, words that are relatively unique to the category occur very infrequently, for various reasons including the use of synonyms, infrequent occurrence of any given named entity (especially in industries that have not been monopolized), and the fact that words that do occur frequently also do so in other categories. Thus, an approach that learned semantic relationships between words may have a better chance of success.

I finally settled on Word2vec, a neural network model. I was not able to find a corpus specific to the kind of documents I'm dealing with (transaction descriptions), so I decided to go with an all-purpose corpus. Common Crawl, which trawls the internet and compiles web content into a humongous corpus, is as general a corpus as one can find. The corpus contains 840 billion documents and would be a real pain to train. Fortunately, a <a href="http://nlp.stanford.edu/projects/glove/">pre-trained GloVe model</a> exists. The principle behind such models is text embedding, in which a 2-layer neural net is fed documents from the corpus and subsequently assigns each word to a point in some high-dimensional space. The semantic similarity between two words in this space can be gauged by the cosine similarity between the two representative points. This allows semantic operations such as the classic "king - queen + woman = man". With this useful tool, I can average the similarities between words in the transaction description and words in the category name, and assign to the transaction the category that gives the highest similarity score. But wait, what about words that occur commonly and are similar to many categories? In the spirit of tf-idf, I normalized the similarity score of a given word to a given category by dividing it by the sum of similarities between the word and the entire set of categories. Another problem is that category names can be quite ambiguous, like "Home Services". Against this problem, I adopted two measures, the first of which was to manually replace ambiguous category names with more specific descriptions, or with specific examples, such as "Housekeeping HVAC Remodeler Handyman Home Improvement Contractor". The second measure is an automated means of doing essentially the same thing as the first measure. Word2vec models are able to produce a collection of words that are most similar to a given set of words. With this functionality, I was able to expand each category name into a larger set of related words.

The input to PaymentClass is the transaction description. Pre-processing included the removal of stopwords like 'payment' and 'purchase', as well as words that were less than two characters long or words that occurred less than 4 times in the entire dataset. After that, the remaining words are run through a spellchecker. <a href="http://pythonhosted.org/pyenchant/">pyEnchant</a> is a pretty good spellchecker which gives multiple suggestions. Each word is checked for existence in the Common Crawl corpus. If it does not exist in the corpus, then the spellchecker is run on the word and subsequently a list of possible replacements is generated. In PaymentClass.Layer2, when the transaction is compared to each of the categories, the replacement word with the highest similarity score to the category is selected during comparison to that category. In other words, the choice of replacement word depends on which category the transaction is being compared to. This methodology allows ambiguity between replacement words to be resolved. 

<h2>Performance</h2>
 
Tuning performance of the algorithm was not straightforward, since the only labelled data available contained miscategorizations in an unknown proportion. Tuning was performed for each of the steps. For the anomaly detection step, a one-class SVM model was trained for each category. All but 1000 entries were used in training. At the risk of possible contamination by mis-categorized entries, the 1000 entries were regarded as inlier data points. Another 1000 entries randomly drawn from other categories were used as outlier data points for the purpose of testing. A gridsearch was performed over the nu and gamma parameters, as well as the choice of kernel. 
At the time of this post, optimization had been completed for just one of the categories, yielding the ROC curve shown below. Optimal parameters were then selected at the inflection point, giving a
precision and recall of 0.85 and 0.75 respectively. 

In the current implementation of PaymentClass.Layer2, the model used is a pre-trained one and not tunable. I tested the model using both equally weighted tokens, as well as tokens weighted by a tf-idf-like scheme, where the similarity to a given category was multiplied by the logarithm of the number of categories to the number of categories for which the similarity was positive. The latter weighting scheme produced a marginally better accuracy of 25% (proportion of test data matched to the correct categories) when tested on 30 previously mis-classified entries (these were rare and had to be hand-picked). The low accuracy was not altogether unexpected. For many previously mis-classified entries, the description did not contain words that were semantically similar to the category descriptions. Instead, a better approach might be to perform a named entity lookup to match business brands to possible categories, e.g. Toyota to "Auto & Transport". To this end, I built into the code a mechanism to allow manual input of keywords which would trigger immediate matching to a specified category or restrict category comparisons to a small subset when found in a transaction description. 

Finally, the entire pipeline, consisting of keyword search, anomaly detection, and finally new category assignment, had an accuracy of 80% when tested on the same test set used for SVM model tuning. By keeping most of the correctly categorized transactions and correcting a portion of the mis-classified ones, the overall performance of PaymentClass is an improvement over what Cinch had previously. There is potential for further improvement as Cinch injects domain expertise into the algorithm through the keyword mechanisms I had implemented, and possibly by using a different text embedding model with either a corpus more specific to the context of transaction descriptions or with the inclusion of bigrams to capture two-word entity names. Another avenue for improvement is re-definition of categories, to either dis-ambiguate overly inclusive categories like 'Home Services' or merge closely related categories like 'Baby Supplies' and 'Toys'.
 
