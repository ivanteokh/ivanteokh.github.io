---
layout: post
title:  "Categorizing Transactions with PaymentClass"
date:   2016-06-16 20:31:30 -0400
#categories: 
---

<h2>Background</h2>

Cinch Financial helps their subscribers to choose the best personal financial products for their needs through an app which combines user financial background, spending history, finance expertise, and unbiased recommendations (Cinch is not paid by any product provider).
Cinch obtains users' spending history from debit/credit card transaction data provided by a third-party vendor. Each transaction has a date and time, a dollar amount, a brief description, and a category assigned by the vendor. For example, if you made a loan payment today, it might look something like "WELLS FARGO HOME BILL PAYM ACH Withdrawal" and be classified as "Mortgage & Rent". The app depends on classifications like this to make sensible recommendations to its users. So what happens if the payment got classified as "Home Services" by mistake? Then in the eyes of the app, you would still carry that debt and have incurred an expenditure. 

Mis-classifications do occur occasionally in the data. Although rare, they can have a significant impact on the reliability of the app. I have been working with Cinch on a data science solution to detect such mis-classifications and to correct them. I was provided with the details of more than 28 million transactions, which had been classified into 300 categories. My approach was to use natural language processing with a number of machine learning models to identify patterns in the descriptions that may accurately predict the right categories.

The data from Cinch presented significant challenges. First of all, it is highly imbalanced across a large number of categories. The charts below show the data point frequencies (blue bars) in the top and bottom 15 categories, which can range from over 4 million to less than 1000. A multi-class approach with bootstrap re-sampling may work in such a scenario, but for another problem. The red bars in the charts show the number of features in each category, obtained by vectorizing unigram tokens generated from the descriptions after pre-processing. While the top 15 categories have nice small features-to-data points ratio, the bottom 15 categories have feature counts that are large compared to the frequencies. That means that the multi-class + bootstrap approach is likely to over-fit some of these categories. Furthermore, the proportion of mis-classified transactions in each category was unknown, although a brief inspection of randomly selected transactions suggest that the majority is correct.

<img src=""><img src="">

My solution is divided into two major steps. In the first step, outliers (transactions whose descriptions are least like others in their category) are picked out by a one-class SVM model, which had been trained on the dataset. Outliers are then fed into a second model, which was pre-trained on an external corpus, and a new category is determined. Before each step, a series of pre-processing treatments were applied to the transaction data. Read on for more details on each of these components. Or you could skip to the next section to learn how well this approach worked.

<h2>Solution</h2>

The first step of the solution was anomaly detection using One-Class SVM. This model assumes that the proportion of incorrect classifications is very small (pedantic point - the formulation actually assumes that all training points are correct classifications). Like a regular SVM, the one-class SVM uses a hypersurface in feature space to define the boundary between two distinct categories. The difference is that in one-class SVM, all the training data belong in one category, and the dividing surface is constructed in such a way as to exclude a user-specified proportion of data points, which are then considered as outliers. As you might already have guessed, I had to train a model for each of the 300 categories. Furthermore, each model would have to be separately tuned for accuracy.

In the second step, a new category is assigned to each outlier. My first attempt was to use topic modeling via Latent Dirichlet Allocation with tf-idf weighting. However, it turns out that the topics did not correlate with the categories most of the time. Upon an inspection of a subset of the data, it seemed to me that very few categories actually had transactions with the same words recurring in their descriptions. These tended to be the large categories, such as Transfers and Fast Food, where highly-recurring names of well-known companies dominate in the descriptions. For most categories, words that are relatively unique to the category occur very infrequently, for various reasons including the use of synonyms, infrequent occurrence of any given named entity (especially in industries that have not been monopolized), and the fact that words that do occur frequently also do so in other categories. Thus, an approach that learned semantic relationships between words may have a better chance of success.

I finally settled on Word2vec, a neural network model. I was not able to find a corpus specific to the kind of documents I'm dealing with (transaction descriptions), so I decided to go with an all-purpose corpus. Common Crawl, which trawls the internet and compiles web content into a humongous corpus, is as general a corpus as one can find. The corpus contains 840 billion documents and would be a real pain to train. Fortunately, a <a href="http://nlp.stanford.edu/projects/glove/">pre-trained GloVe model</a> exists, which I used for my purposes. The principle behind such models is text embedding, in which a 2-layer neural net is fed documents from the corpus and subsequently assigns each word to a point in some high-dimensional space. The semantic similarity between two words in this space can be gauged by the cosine similarity between the two representative points. This allows semantic operations such as the classic "king - queen + woman = man". With this useful tool, I can average the similarities between words in the transaction description and words in the category name, and assign to the transaction the category that gives the highest similarity score. But wait, what about words that occur commonly and are similar to many categories? In the spirit of tf-idf, I normalized the similarity score of a given word to a given category by dividing it by the sum of similarities between the word and the entire set of categories. Another problem is that category names can be quite ambiguous, like "Home Services". Against this problem, I adopted two measures, the first of which was to manually replace ambiguous category names with more specific descriptions, or with specific examples, such as "Housekeeping HVAC Remodeler Handyman Home Improvement Contractor". The second measure is an automated means of doing essentially the same thing as the first measure. Word2vec models are able to produce a collection of words that are most similar to a given set of words. With this functionality, I was able to expand each category name into a larger set of related words.

The input to PaymentClass is the transaction description. Pre-processing included the removal of stopwords like 'payment' and 'purchase', as well as words that were less than two characters long or words that occurred less than 4 times in the entire dataset. After that, the remaining words are run through a spellchecker. <a href="http://pythonhosted.org/pyenchant/">pyEnchant</a> is a pretty good spellchecker which gives multiple suggestions. Each word is checked for existence in the Common Crawl corpus. If it does not exist in the corpus, then the spellchecker is run on the word and subsequently a list of possible replacements is generated. During the second step, when the transaction is compared to each of the categories, the replacement word with the highest similarity score to the category is selected during comparison to that category. In other words, the choice of replacement word depends on which category the transaction is being compared to. This methodology allows ambiguity between replacement words to be resolved. 


 

